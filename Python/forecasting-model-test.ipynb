{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                timestamp      value\n",
      "0    07/09/2024 17:00:00  14.312299\n",
      "1    07/09/2024 17:00:15  10.533495\n",
      "2    07/09/2024 17:00:30  11.239361\n",
      "3    07/09/2024 17:00:45  11.239361\n",
      "4    07/09/2024 17:01:00   9.610936\n",
      "..                   ...        ...\n",
      "276  07/09/2024 18:09:00   0.012682\n",
      "277  07/09/2024 18:09:15   0.011093\n",
      "278  07/09/2024 18:09:30   0.015599\n",
      "279  07/09/2024 18:09:45   0.012942\n",
      "280  07/09/2024 18:10:00   0.015930\n",
      "\n",
      "[281 rows x 2 columns]>\n",
      "241\n",
      "(168, 40, 1)\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_7 (Bidirecti  (None, 40, 100)           20800     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 40, 80)            45120     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirecti  (None, 60)                26640     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 60)                240       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 30)                1830      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94661 (369.77 KB)\n",
      "Trainable params: 94541 (369.30 KB)\n",
      "Non-trainable params: 120 (480.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 53s 1s/step - loss: 0.8760 - val_loss: 0.9495 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.2575 - val_loss: 0.9365 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 358ms/step - loss: 0.1921 - val_loss: 0.9537 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.1189 - val_loss: 0.9576 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 0.1328 - val_loss: 0.9659 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 423ms/step - loss: 0.1813 - val_loss: 0.9786 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 335ms/step - loss: 0.1213 - val_loss: 0.9622 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.1062 - val_loss: 0.9514 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.0931 - val_loss: 0.9424 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 322ms/step - loss: 0.1117 - val_loss: 0.9397 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 325ms/step - loss: 0.1178 - val_loss: 0.9377 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.0992 - val_loss: 0.9198 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 433ms/step - loss: 0.1232 - val_loss: 0.9066 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 360ms/step - loss: 0.1163 - val_loss: 0.9055 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 437ms/step - loss: 0.0821 - val_loss: 0.9059 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.0733 - val_loss: 0.8954 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.0811 - val_loss: 0.8863 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 297ms/step - loss: 0.0930 - val_loss: 0.8870 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 0.0736 - val_loss: 0.8751 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 0.1012 - val_loss: 0.8652 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.0795 - val_loss: 0.8547 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 366ms/step - loss: 0.0819 - val_loss: 0.8404 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 346ms/step - loss: 0.0792 - val_loss: 0.8561 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.0867 - val_loss: 0.8579 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 472ms/step - loss: 0.0935 - val_loss: 0.8313 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 505ms/step - loss: 0.0510 - val_loss: 0.8059 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 411ms/step - loss: 0.0794 - val_loss: 0.8010 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 3s 558ms/step - loss: 0.0995 - val_loss: 0.7979 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 3s 506ms/step - loss: 0.0877 - val_loss: 0.8006 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.0790 - val_loss: 0.8122 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 364ms/step - loss: 0.0736 - val_loss: 0.8004 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.0520 - val_loss: 0.7702 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.0661 - val_loss: 0.7510 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 396ms/step - loss: 0.0610 - val_loss: 0.7344 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 371ms/step - loss: 0.0815 - val_loss: 0.7268 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.0772 - val_loss: 0.7404 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.1473 - val_loss: 0.7339 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.0792 - val_loss: 0.7021 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 353ms/step - loss: 0.0699 - val_loss: 0.6945 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 360ms/step - loss: 0.0804 - val_loss: 0.7050 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 341ms/step - loss: 0.0814 - val_loss: 0.6917 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 364ms/step - loss: 0.0751 - val_loss: 0.6642 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.0648 - val_loss: 0.6547 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 443ms/step - loss: 0.0949 - val_loss: 0.6402 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 364ms/step - loss: 0.1095 - val_loss: 0.6370 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 0.0890 - val_loss: 0.6619 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 2s 323ms/step - loss: 0.0988 - val_loss: 0.6515 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 2s 421ms/step - loss: 0.0824 - val_loss: 0.6513 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.0727 - val_loss: 0.6690 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 2s 456ms/step - loss: 0.1056 - val_loss: 0.6325 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 2s 452ms/step - loss: 0.0710 - val_loss: 0.6012 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 2s 496ms/step - loss: 0.0898 - val_loss: 0.5959 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 3s 502ms/step - loss: 0.0729 - val_loss: 0.6021 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 3s 625ms/step - loss: 0.0877 - val_loss: 0.6097 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 2s 431ms/step - loss: 0.0635 - val_loss: 0.5812 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.0696 - val_loss: 0.5361 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 2s 414ms/step - loss: 0.0718 - val_loss: 0.5230 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 2s 421ms/step - loss: 0.0812 - val_loss: 0.5245 - lr: 5.0000e-04\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.0806 - val_loss: 0.5144 - lr: 5.0000e-04\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 2s 359ms/step - loss: 0.0637 - val_loss: 0.5119 - lr: 5.0000e-04\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 2s 338ms/step - loss: 0.0746 - val_loss: 0.5123 - lr: 5.0000e-04\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.0567 - val_loss: 0.5142 - lr: 5.0000e-04\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 2s 359ms/step - loss: 0.0754 - val_loss: 0.5295 - lr: 5.0000e-04\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.0604 - val_loss: 0.5240 - lr: 5.0000e-04\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 2s 360ms/step - loss: 0.0685 - val_loss: 0.4899 - lr: 5.0000e-04\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.0613 - val_loss: 0.4941 - lr: 5.0000e-04\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 2s 334ms/step - loss: 0.0679 - val_loss: 0.4969 - lr: 5.0000e-04\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 2s 345ms/step - loss: 0.0579 - val_loss: 0.5263 - lr: 5.0000e-04\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 2s 396ms/step - loss: 0.0678 - val_loss: 0.5374 - lr: 5.0000e-04\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.0667 - val_loss: 0.5305 - lr: 5.0000e-04\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.0639 - val_loss: 0.5223 - lr: 2.5000e-04\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 0.0469 - val_loss: 0.5093 - lr: 2.5000e-04\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 0.0668 - val_loss: 0.5114 - lr: 2.5000e-04\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 2s 450ms/step - loss: 0.0519 - val_loss: 0.5080 - lr: 2.5000e-04\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 2s 455ms/step - loss: 0.0650 - val_loss: 0.4949 - lr: 2.5000e-04\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 5.2245\n",
      "Mean Square Error: 5.2245025634765625\n",
      "Mean Square Error: 3.1757106773007537\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from tensorflow import keras\n",
    "\n",
    "def prepare_data(seq,num):\n",
    "  x=[]\n",
    "  y=[]\n",
    "  for i in range(0,(len(seq)-num),1):\n",
    "    \n",
    "    input_ = seq[i:i+num]\n",
    "    output  = seq[i+num]\n",
    "    \n",
    "    x.append(input_)\n",
    "    y.append(output)\n",
    "    \n",
    "  return np.array(x), np.array(y)\n",
    "\n",
    "data=pd.read_csv('../Utilities\\csv\\metrics_2024-09-07\\metrics.csv')\n",
    "print(data.head)\n",
    "cpu_usage = data['value'].values\n",
    "\n",
    "num=40 # The first 10 mins of traffic\n",
    "sample = cpu_usage[:num]\n",
    "\n",
    "x,y= prepare_data(cpu_usage,num)\n",
    "print(len(x))\n",
    "\n",
    "ind = int(0.7 * len(x))\n",
    "x_tr = x[:ind]\n",
    "y_tr = y[:ind]\n",
    "x_val=x[ind:]\n",
    "y_val=y[ind:]\n",
    "\n",
    "#normalize the inputs\n",
    "x_scaler= StandardScaler()\n",
    "x_tr = x_scaler.fit_transform(x_tr)\n",
    "x_val= x_scaler.transform(x_val)\n",
    "\n",
    "#reshaping the output for normalization\n",
    "y_tr=y_tr.reshape(len(y_tr),1)\n",
    "y_val=y_val.reshape(len(y_val),1)\n",
    "\n",
    "#normalize the output\n",
    "y_scaler=StandardScaler()\n",
    "y_tr = y_scaler.fit_transform(y_tr)[:,0]\n",
    "y_val = y_scaler.transform(y_val)[:,0]\n",
    "\n",
    "#reshaping input data\n",
    "x_tr= x_tr.reshape(x_tr.shape[0],x_tr.shape[1],1)\n",
    "x_val= x_val.reshape(x_val.shape[0],x_val.shape[1],1)\n",
    "print(x_tr.shape)\n",
    "\n",
    "def traffic_prediction_lstm():\n",
    "    model = Sequential()\n",
    "    # Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), input_shape=(num, 1)))\n",
    "    # Additional LSTM layers\n",
    "    model.add(LSTM(40, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(30, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    # Dense layer with ReLU activation\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    # Output layer for regression\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # Learning Rate Scheduler\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    return model, lr_scheduler\n",
    "\n",
    "\n",
    "lstm_model, lr_scheduler = traffic_prediction_lstm()\n",
    "# Early Stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = lstm_model.fit(x_tr, y_tr,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "mse = lstm_model.evaluate(x_val,y_val)\n",
    "print(\"Mean Square Error:\",mse)\n",
    "\n",
    "#build a simple model\n",
    "def compute_moving_average(data):\n",
    "  pred=[]\n",
    "  for i in data:\n",
    "    avg=np.sum(i)/len(i)\n",
    "    pred.append(avg)\n",
    "  return np.array(pred)\n",
    "x_reshaped = x_val.reshape(-1,40)\n",
    "y_pred = compute_moving_average(x_reshaped)\n",
    "mse = np.sum ( (y_val - y_pred) **2 ) / (len(y_val))\n",
    "print(\"Mean Square Error:\",mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
