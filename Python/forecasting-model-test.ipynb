{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'anaconda3 (Python 3.11.3)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import *\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import *\n",
    "from scipy import interpolate\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pandas import to_numeric\n",
    "from hmmlearn import hmm\n",
    "\n",
    "def create_time_features(data):\n",
    "    # Assume 'cpu_data' is your dataframe and 'datetime' is the column with datetime values\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])  # Ensure it's a datetime type\n",
    "    data['hour'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    data['day_of_month'] = data['datetime'].dt.day\n",
    "    data['is_weekend'] = data['datetime'].dt.weekday >= 5  # True for Saturday and Sunday\n",
    "    return data\n",
    "\n",
    "# Define a function to create lagged features\n",
    "def create_additional_features(df, target_col, lags):\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    df['cpu_change'] = df['cpu_usage'].diff() # Change in CPU usage\n",
    "    df['moving_avg'] = df['cpu_usage'].rolling(window=10).mean() # 10-minute moving average\n",
    "    return df\n",
    "\n",
    "cpu_data = pd.read_csv('../Utilities\\csv/trainingset.csv')\n",
    "cpu_data['machine_id'] = pd.Categorical(cpu_data['machine_id']).codes + 1\n",
    "cpu_data = create_time_features(cpu_data)\n",
    "lags = [5, 10, 60]\n",
    "cpu_data = create_additional_features(cpu_data, 'cpu_usage', lags)\n",
    "\n",
    "# Drop any rows with NaN values that were created due to shifting\n",
    "cpu_data.dropna(inplace=True)\n",
    "\n",
    "column_to_move = cpu_data.pop('cpu_usage')\n",
    "cpu_data.insert(1, 'cpu_usage', column_to_move)\n",
    "all_features_no_timestamp = cpu_data.columns[1:len(cpu_data.columns)]\n",
    "features_for_hmm = cpu_data[all_features_no_timestamp].values\n",
    "\n",
    "num_states = 10\n",
    "model = hmm.GaussianHMM(n_components=num_states, covariance_type=\"full\", n_iter=1000)\n",
    "\n",
    "model.fit(features_for_hmm)\n",
    "\n",
    "hidden_states = model.predict(features_for_hmm)\n",
    "\n",
    "cpu_data['hidden_state'] = hidden_states\n",
    "\n",
    "cpu_data.head\n",
    "\n",
    "file_path = \"../Utilities/csv/cpu_usage_train.csv\"\n",
    "cpu_data.to_csv(file_path, index=False)\n",
    "\n",
    "for machine_id, machine_data in cpu_data.groupby('machine_id'):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(machine_data['cpu_usage'], 'g-', label='CPU Usage')\n",
    "    ax.tick_params('y', colors='g')\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('CPU Usage')\n",
    "    ax.set_ylim(0, 100)\n",
    "    plt.title(f'CPU Usage for machine {machine_id} over Time')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for tracking cooldown periods and consecutive low CPU usage\n",
    "scale_up_cooldown = 5  # No scale-up allowed for 5 timesteps after the last scale-up\n",
    "scale_down_consecutive = 2  # CPU must be below threshold for 2 consecutive timesteps to scale down\n",
    "\n",
    "initial_replicas = 1\n",
    "threshold = 70  # SLA threshold at 70%\n",
    "\n",
    "# Function to generate replicas for each machine separately\n",
    "def generate_replicas_for_machine(machine_data):\n",
    "    time_since_last_scale_up = scale_up_cooldown  # Initialize at cooldown to allow immediate scaling if needed\n",
    "    consecutive_below_threshold = 0  # Track consecutive timesteps below threshold\n",
    "    current_replicas = initial_replicas  # Start with initial number of replicas\n",
    "\n",
    "    replicas = []  # List to store the replicas for this machine\n",
    "\n",
    "    # Iterate through CPU usage data for this machine\n",
    "    for cpu_usage in machine_data['cpu_usage']:\n",
    "        # Check if scaling up is allowed\n",
    "        if cpu_usage >= threshold:\n",
    "            if time_since_last_scale_up >= scale_up_cooldown:\n",
    "                current_replicas += 1  # Scale up\n",
    "                time_since_last_scale_up = 0  # Reset cooldown\n",
    "            else:\n",
    "                time_since_last_scale_up += 1  # Increment cooldown timer\n",
    "            consecutive_below_threshold = 0  # Reset consecutive below counter\n",
    "\n",
    "        # Check for scaling down\n",
    "        else:\n",
    "            consecutive_below_threshold += 1\n",
    "            if consecutive_below_threshold >= scale_down_consecutive and current_replicas > 1:\n",
    "                current_replicas -= 1  # Scale down\n",
    "                consecutive_below_threshold = 0  # Reset consecutive counter\n",
    "            time_since_last_scale_up += 1  # Continue cooldown for scale up\n",
    "\n",
    "        # Append the current number of replicas to the list\n",
    "        replicas.append(current_replicas)\n",
    "\n",
    "    # Add the 'replicas' column to the machine data\n",
    "    machine_data['replicas'] = replicas\n",
    "    return machine_data\n",
    "\n",
    "# Group by 'machine_id' and apply the replica generation function to each group\n",
    "cpu_data_with_replicas = cpu_data.groupby('machine_id', group_keys=False).apply(generate_replicas_for_machine)\n",
    "\n",
    "# Reset the index after applying groupby\n",
    "cpu_data_with_replicas.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Plot replicas and CPU usage for each machine\n",
    "for machine_id, machine_data in cpu_data_with_replicas.groupby('machine_id', group_keys=False):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax1.plot(machine_data['cpu_usage'], 'g-', label='CPU Usage')\n",
    "    ax1.set_xlabel('Timestamps')\n",
    "    ax1.set_ylabel('CPU Usage (%)', color='g')\n",
    "    ax1.tick_params('y', colors='g')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(machine_data['replicas'], 'r-', label='Replicas')\n",
    "    ax2.set_ylabel('Replicas')\n",
    "    ax2.tick_params('y')\n",
    "    ax2.set_ylim(1, 5)\n",
    "    \n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.title(f'Replica Scaling for Machine {machine_id}')\n",
    "    plt.show()\n",
    "\n",
    "# Save the dataset with replicas to a CSV file\n",
    "file_path = \"../Utilities/csv/cpu_replicas.csv\"\n",
    "cpu_data_with_replicas.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "cpu_data = pd.read_csv('../Utilities/csv/cpu_usage_train.csv')\n",
    "\n",
    "# Initialize scalers\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Window size (e.g., past 60 time steps) and prediction horizon (next 10 timesteps)\n",
    "window_size = 60\n",
    "prediction_horizon = 10\n",
    "\n",
    "# Function to generate LSTM sequences\n",
    "def create_lstm_sequences(group_data, window_size, prediction_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(group_data) - window_size - prediction_horizon + 1):\n",
    "        # Features are all columns except 'datetime', 'cpu_usage', and 'machine_id'\n",
    "        X.append(group_data.iloc[i:i + window_size].drop(columns=['datetime', 'cpu_usage', 'machine_id']).values)\n",
    "        # Target is the next 10 CPU usage values\n",
    "        y.append(group_data.iloc[i + window_size:i + window_size + prediction_horizon]['cpu_usage'].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "all_features_no_timestamp_cpu_usage = cpu_data.columns[2:len(cpu_data.columns)]\n",
    "# Scale features and target (fit on the entire dataset)\n",
    "cpu_data[all_features_no_timestamp_cpu_usage] = feature_scaler.fit_transform(cpu_data[all_features_no_timestamp_cpu_usage])\n",
    "cpu_data[[\"cpu_usage\"]] = target_scaler.fit_transform(cpu_data[[\"cpu_usage\"]])\n",
    "\n",
    "# Train/test split ratios\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Lists to store train/test data\n",
    "X_train_all, y_train_all = [], []\n",
    "X_test_all, y_test_all = [], []\n",
    "\n",
    "# Group by machine_id\n",
    "for machine_id, group_data in cpu_data.groupby('machine_id'):\n",
    "    # Split the data into train and test sets for this machine\n",
    "    train_data, test_data = train_test_split(group_data, test_size=1 - train_ratio, shuffle=False)\n",
    "    \n",
    "    # Generate moving windows for the training set\n",
    "    X_train, y_train = create_lstm_sequences(train_data, window_size, prediction_horizon)\n",
    "    X_train_all.append(X_train)\n",
    "    y_train_all.append(y_train)\n",
    "    \n",
    "    # Generate moving windows for the test set\n",
    "    X_test, y_test = create_lstm_sequences(test_data, window_size, prediction_horizon)\n",
    "    X_test_all.append(X_test)\n",
    "    y_test_all.append(y_test)\n",
    "\n",
    "# Combine all the machine-specific windows together for training and testing\n",
    "X_train_all = np.concatenate(X_train_all, axis=0)\n",
    "y_train_all = np.concatenate(y_train_all, axis=0)\n",
    "X_test_all = np.concatenate(X_test_all, axis=0)\n",
    "y_test_all = np.concatenate(y_test_all, axis=0)\n",
    "\n",
    "# Print the shapes of the final train/test data\n",
    "print(\"Train shapes:\", X_train_all.shape, y_train_all.shape)\n",
    "print(\"Test shapes:\", X_test_all.shape, y_test_all.shape)\n",
    "\n",
    "# Build the enhanced CNN-LSTM model\n",
    "lstm_model = Sequential()\n",
    "# CNN layers\n",
    "lstm_model.add(Conv1D(filters=240, kernel_size=3, activation='relu', input_shape=(X_train_all.shape[1], X_train_all.shape[2])))\n",
    "lstm_model.add(BatchNormalization())  # Normalize activations\n",
    "lstm_model.add(Conv1D(filters=120, kernel_size=3, activation='relu'))\n",
    "lstm_model.add(BatchNormalization())  # Normalize activations\n",
    "lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "# LSTM layers\n",
    "lstm_model.add(LSTM(units=250, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(units=100, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(LSTM(units=50, return_sequences=False))\n",
    "# Output layer (multi-step prediction)\n",
    "lstm_model.add(Dense(prediction_horizon))\n",
    "\n",
    "# Early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model_test.hdf5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "# Learning rate reduction callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "lstm_model.compile(optimizer=\"adam\", loss='huber_loss')  # Use Huber loss to reduce outliers' impact\n",
    "lstm_model.summary()\n",
    "# Train the model\n",
    "history = lstm_model.fit(X_train_all, y_train_all, epochs=50, batch_size=64, validation_data=(X_test_all, y_test_all),\n",
    "                         verbose=1, callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "mse_lstm = lstm_model.evaluate(X_test_all, y_test_all)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "\n",
    "print(f\"RMSE: {rmse_lstm}, MSE: {mse_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model('best_model_test.keras')\n",
    "# Make predictions\n",
    "y_pred = lstm_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions using the target scaler\n",
    "y_pred_unscaled = target_scaler.inverse_transform(y_pred)\n",
    "\n",
    "# If `y_test` also needs inverse transforming\n",
    "y_test_unscaled = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "# Calculate MSE and RMSE\n",
    "mse_lstm = mean_squared_error(y_test_unscaled[0], y_pred_unscaled[0])\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "\n",
    "print(f\"RMSE: {rmse_lstm}, MSE: {mse_lstm}\")\n",
    "\n",
    "# Plotting Actual vs Predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_unscaled[0], label='Actual Values', color='blue', alpha=0.6)\n",
    "plt.plot(y_pred_unscaled[0], label='Predicted Values', color='orange', alpha=0.6)\n",
    "plt.title('Actual vs Predicted CPU Usage (10 Minutes)')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('CPU Usage')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test_unscaled, y_pred_unscaled)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "\n",
    "print(f\"RMSE: {rmse_lstm}, MSE: {mse_lstm}\")\n",
    "# Plotting Actual vs Predicted values\n",
    "y_test_rescaled_flat = y_test_unscaled.flatten()\n",
    "predictions_rescaled_flat = y_pred_unscaled.flatten()\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_rescaled_flat, label='Actual Values', color='blue', alpha=0.6)\n",
    "plt.plot(predictions_rescaled_flat, label='Predicted Values', color='orange', alpha=0.6)\n",
    "plt.title('Actual vs Predicted CPU Usage')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('CPU Usage')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# BEST MODEL (best_model.hdf5): RMSE: 1.8922554032436125, MSE: 3.580630511104647\n",
    "# ADDED: Made HMM use all datapoints to predict, num states = 10, used gridSearch\n",
    "# IDEA: Make HMM only use CPU usage as this means it should relate the states to changes in CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting number of replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "replicas_data = pd.read_csv('../Utilities\\csv/cpu_replicas.csv')\n",
    "\n",
    "# Split the data\n",
    "X = replicas_data[['cpu_usage']]\n",
    "y = replicas_data['replicas']\n",
    "X_train_replicas, X_test_replicas, y_train_replicas, y_test_replicas = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for decision tree\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, scoring='r2', verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_replicas.values, y_train_replicas.values)\n",
    "\n",
    "# Best parameters found\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Predict with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_replicas = best_model.predict(X_test_replicas)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test_replicas, y_pred_replicas)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(y_test_replicas, y_pred_replicas, color='blue')\n",
    "plt.plot([y_test_replicas.min(), y_test_replicas.max()], [y_test_replicas.min(), y_test_replicas.max()], color='red', lw=2)\n",
    "plt.xlabel('Actual Number of Replicas')\n",
    "plt.ylabel('Predicted Number of Replicas')\n",
    "plt.title('Decision Tree: Actual vs Predicted Replicas')\n",
    "plt.show()\n",
    "\n",
    "# Predict using LSTM output\n",
    "predictions = lstm_model.predict(X_test)\n",
    "# cpu_usage_scaler = MinMaxScaler()\n",
    "# cpu_usage_scaler.min_, cpu_usage_scaler.scale_ = scaler.min_[-1], scaler.scale_[-1]\n",
    "\n",
    "predictions_rescaled = target_scaler.inverse_transform(predictions)\n",
    "\n",
    "future_replicas = best_model.predict(predictions_rescaled.reshape(-1, 1))\n",
    "print(f'Predicted number of replicas for 10 mins future CPU usage ({predictions_rescaled[0][-1]}): {future_replicas[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Burst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from kubernetes import client, config\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_time_range():\n",
    "    end_time = int(time.time())  # Current timestamp in seconds\n",
    "    start_time = end_time - (3 * 3600) # Hours in seconds\n",
    "    return start_time, end_time\n",
    "\n",
    "# Prometheus server URL (adjust according to your setup)\n",
    "PROMETHEUS_URL = \"http://localhost:9090\"\n",
    "\n",
    "def query_prometheus_range(query, start_time, end_time, step):\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'start': start_time,\n",
    "        'end': end_time,\n",
    "        'step': step  # Interval between points (e.g., 60s = 1 minute intervals)\n",
    "    }\n",
    "    response = requests.get(f\"{PROMETHEUS_URL}/api/v1/query_range\", params=params)\n",
    "    return response.json()\n",
    "\n",
    "# Function to get CPU usage for a pod over the past hour\n",
    "def get_pod_cpu_usage_range(pod_name, start_time, end_time):\n",
    "    # Prometheus query to get CPU usage over the last hour (rate over 1 minute)\n",
    "    query = f'avg((sum(rate(container_cpu_usage_seconds_total{{pod=~\"{pod_name}-.*\"}}[1m])) by (pod)) / (sum(kube_pod_container_resource_limits{{pod=~\"{pod_name}-.*\", resource=\"cpu\"}}) by (pod))) * 100'\n",
    "\n",
    "    # Query Prometheus with a 60-second step interval\n",
    "    result = query_prometheus_range(query, start_time, end_time, step=\"60\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to convert the Prometheus result to a pandas DataFrame\n",
    "def prometheus_to_dataframe(prometheus_result):\n",
    "    if not prometheus_result or 'data' not in prometheus_result:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if there's no data\n",
    "\n",
    "    # Extract the 'values' from the first result (assuming only one pod)\n",
    "    values = prometheus_result['data']['result'][0]['values']\n",
    "\n",
    "    # Create a DataFrame from the values\n",
    "    df = pd.DataFrame(values, columns=['timestamp', 'value'])\n",
    "\n",
    "    # Convert UNIX timestamp to human-readable datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n",
    "    # Convert the value column to float\n",
    "    df['value'] = df['value'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Get weather data\n",
    "def get_weather_data(start_time, end_time):\n",
    "    API_KEY = 'DbiSeuEozdnpBiOOJ25UH66tyRj17LDx'\n",
    "    LOCATION = \"51.507407,-0.12772404\"  # London\n",
    "\n",
    "    start_time = datetime.utcfromtimestamp(start_time).isoformat() + \"Z\"\n",
    "    end_time = datetime.utcfromtimestamp(end_time).isoformat() + \"Z\"\n",
    "\n",
    "    # Build the API URL for the Timeline API\n",
    "    URL = f\"https://api.tomorrow.io/v4/timelines?apikey={API_KEY}\"\n",
    "\n",
    "    # Define the query parameters\n",
    "    query_params = {\n",
    "        \"location\": LOCATION,\n",
    "        \"fields\": [\"temperature\", \"humidity\", \"windSpeed\"],\n",
    "        \"units\": \"metric\",  # For Celsius and kph\n",
    "        \"timesteps\": \"1m\",  # 1-minute intervals\n",
    "        \"startTime\": start_time,\n",
    "        \"endTime\": end_time,\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(URL, params=query_params)\n",
    "    data = response.json()\n",
    "    # Check for a successful response\n",
    "    if response.status_code == 200:\n",
    "        # Create a list to store the data\n",
    "        weather_data = []\n",
    "        \n",
    "        # Loop through each interval and extract the weather data\n",
    "        for interval in data[\"data\"][\"timelines\"][0][\"intervals\"]:\n",
    "            datetime_obj = datetime.strptime(interval[\"startTime\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            formatted_timestamp = datetime_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            weather_record = {\n",
    "                \"time\": formatted_timestamp,\n",
    "                \"temperature\": interval[\"values\"][\"temperature\"],\n",
    "                \"humidity\": interval[\"values\"][\"humidity\"],\n",
    "                \"wind_speed\": interval[\"values\"][\"windSpeed\"],\n",
    "            }\n",
    "            weather_data.append(weather_record)\n",
    "        \n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(weather_data)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {data['message']}\")\n",
    "    return df\n",
    "\n",
    "    API_KEY = 'DbiSeuEozdnpBiOOJ25UH66tyRj17LDx'\n",
    "    LOCATION = \"51.507407,-0.12772404\"  # London\n",
    "\n",
    "    start_time = datetime.utcfromtimestamp(start_time).isoformat() + \"Z\"\n",
    "    end_time = datetime.utcfromtimestamp(end_time).isoformat() + \"Z\"\n",
    "\n",
    "    # Build the API URL for the Timeline API\n",
    "    URL = f\"https://api.tomorrow.io/v4/timelines?apikey={API_KEY}\"\n",
    "\n",
    "    # Define the query parameters\n",
    "    query_params = {\n",
    "        \"location\": LOCATION,\n",
    "        \"fields\": [\"temperature\", \"humidity\", \"windSpeed\"],\n",
    "        \"units\": \"metric\",  # For Celsius and kph\n",
    "        \"timesteps\": \"1m\",  # 1-minute intervals\n",
    "        \"startTime\": start_time,\n",
    "        \"endTime\": end_time,\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(URL, params=query_params)\n",
    "    data = response.json()\n",
    "    # Check for a successful response\n",
    "    if response.status_code == 200:\n",
    "        # Create a list to store the data\n",
    "        weather_data = []\n",
    "        \n",
    "        # Loop through each interval and extract the weather data\n",
    "        for interval in data[\"data\"][\"timelines\"][0][\"intervals\"]:\n",
    "            datetime_obj = datetime.strptime(interval[\"startTime\"], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            formatted_timestamp = datetime_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            weather_record = {\n",
    "                \"time\": formatted_timestamp,\n",
    "                \"temperature\": interval[\"values\"][\"temperature\"],\n",
    "                \"humidity\": interval[\"values\"][\"humidity\"],\n",
    "                \"wind_speed\": interval[\"values\"][\"windSpeed\"],\n",
    "            }\n",
    "            weather_data.append(weather_record)\n",
    "        \n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(weather_data)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {data['message']}\")\n",
    "    return df\n",
    "\n",
    "def get_dataset():\n",
    "    start_time, end_time = get_time_range()\n",
    "    weather_data_1h = get_weather_data(start_time, end_time)\n",
    "    pod_cpu_data = get_pod_cpu_usage_range(\"microsvc\", start_time, end_time)\n",
    "\n",
    "    # Convert the data to a pandas DataFrame\n",
    "    cluster_data_1h = prometheus_to_dataframe(pod_cpu_data)\n",
    "\n",
    "    cluster_data_1h['timestamp'] = pd.to_datetime(cluster_data_1h['timestamp']).dt.floor('T')\n",
    "    weather_data_1h['time'] = pd.to_datetime(weather_data_1h['time']).dt.floor('T')\n",
    "    full_1h_data = pd.merge(cluster_data_1h, weather_data_1h, left_on='timestamp', right_on='time')\n",
    "    full_1h_data = full_1h_data.drop('time', axis=1)\n",
    "    full_1h_data = full_1h_data.drop('timestamp', axis=1)\n",
    "    full_1h_data.loc[:, 'temperature_lag_10_mins'] = full_1h_data['temperature'].shift(10)\n",
    "    full_1h_data.loc[:, 'temperature_lag_1_hour'] = full_1h_data['temperature'].shift(60)\n",
    "    full_1h_data.dropna(inplace=True)\n",
    "    return full_1h_data\n",
    "\n",
    "def create_sequences_multi_step(data, seq_length, pred_steps):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(len(data) - seq_length - pred_steps + 1):\n",
    "        sequences.append(data[i:i + seq_length, :-1])  # Features (last hour of data)\n",
    "        labels.append(data[i + seq_length:i + seq_length + pred_steps, -1])  # Predict next 10 values\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Load kube config\n",
    "config.load_kube_config()\n",
    "# If running inside the cluster, use:\n",
    "# config.load_incluster_config()\n",
    "\n",
    "def scale_out(additional_replicas, deployment_name, namespace='default'):\n",
    "    api = client.AppsV1Api()\n",
    "    deployment = api.read_namespaced_deployment(deployment_name, namespace)\n",
    "    current_replicas = deployment.spec.replicas\n",
    "    new_replicas = current_replicas + additional_replicas\n",
    "    deployment.spec.replicas = new_replicas\n",
    "    api.patch_namespaced_deployment(deployment_name, namespace, deployment)\n",
    "    print(f\"Scaled out to {new_replicas} replicas.\")\n",
    "    return new_replicas\n",
    "\n",
    "def scale_in(remove_replicas, deployment_name, namespace='default'):\n",
    "    api = client.AppsV1Api()\n",
    "    deployment = api.read_namespaced_deployment(deployment_name, namespace)\n",
    "    current_replicas = deployment.spec.replicas\n",
    "    new_replicas = max(current_replicas - remove_replicas, 1)\n",
    "    deployment.spec.replicas = new_replicas\n",
    "    api.patch_namespaced_deployment(deployment_name, namespace, deployment)\n",
    "    print(f\"Scaled in to {new_replicas} replicas.\")\n",
    "    return new_replicas\n",
    "\n",
    "def detect_burst(monitoring_interval, window_size, resource_prediction_model, replica_prediction_model, replicas):\n",
    "    is_burst = False\n",
    "    replicas_before_burst = 1\n",
    "    past_predictions = np.array([])\n",
    "    feature_scaler_burst = MinMaxScaler()\n",
    "    # while True:\n",
    "    x = 0\n",
    "    while x < 5:\n",
    "        x += 1\n",
    "        time.sleep(monitoring_interval)\n",
    "        full_3h_data = get_dataset()\n",
    "        scaled_data = feature_scaler_burst.fit_transform(full_3h_data)\n",
    "        # In 10 minute increments\n",
    "        prediction_steps = 10\n",
    "        prediction_lookup = 60\n",
    "        predict_data, predict_labels = create_sequences_multi_step(scaled_data, prediction_lookup, prediction_steps)\n",
    "        cpu_predictions = resource_prediction_model.predict(predict_data)\n",
    "        predictions_rescaled = feature_scaler_burst.inverse_transform(cpu_predictions)\n",
    "        n_predicted = replica_prediction_model.predict(predictions_rescaled.reshape(-1, 1))\n",
    "        past_predictions = np.append(past_predictions, n_predicted)\n",
    "        sd_max = 0\n",
    "        n_max = 0\n",
    "        for i in range(1, window_size + 1):\n",
    "            sigma_i = np.std(past_predictions)\n",
    "\n",
    "            if sigma_i > sd_max:\n",
    "                sd_max = sigma_i\n",
    "                n_max = max(past_predictions.flatten())\n",
    "\n",
    "        if sd_max >= 2 and not is_burst:\n",
    "            # Detected burst, increase replicas to n_max\n",
    "            replicas_during_burst = n_max\n",
    "            is_burst = True\n",
    "            replicas_before_burst = n_predicted  # Store current predicted replicas before burst\n",
    "        elif sd_max >= 2 and is_burst:\n",
    "            # Continuation of the burst\n",
    "            replicas_during_burst = n_max\n",
    "        elif sd_max < 2 and is_burst:\n",
    "            if replicas_before_burst > n_predicted:\n",
    "                # Burst ending, scale back to predicted replicas\n",
    "                replicas_during_burst = n_predicted\n",
    "                is_burst = False\n",
    "                replicas_before_burst = 1\n",
    "            else:\n",
    "                # Keep scaling at n_max\n",
    "                replicas_during_burst = n_max\n",
    "        else:\n",
    "            # Normal condition, set replicas to predicted\n",
    "            replicas_during_burst = n_predicted[0]\n",
    "\n",
    "        current_replica_count = int(replicas)\n",
    "        replicas_during_burst = int(replicas_during_burst)\n",
    "\n",
    "        if current_replica_count < replicas_during_burst:\n",
    "            additional_replicas = int(replicas_during_burst - current_replica_count)\n",
    "            replicas = scale_out(additional_replicas, 'microsvc', namespace='default')\n",
    "        elif current_replica_count > replicas_during_burst:\n",
    "            replica_difference = int(current_replica_count - replicas_during_burst)\n",
    "            replicas = scale_in(replica_difference, 'microsvc', namespace='default')\n",
    "        else:\n",
    "            print(\"No replica update needed\")\n",
    "        # Log the updated replica count\n",
    "        print(f\"Updated replica count: {replicas}\")\n",
    "\n",
    "\n",
    "api = client.AppsV1Api()\n",
    "deployment = api.read_namespaced_deployment('microsvc', namespace='default')\n",
    "replicas = deployment.spec.replicas\n",
    "print(f\"Original replica count: {replicas}\")\n",
    "detect_burst(60, 10, lstm_model, best_model, replicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HPA System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# joblib.dump(model, 'cpu_usage_predictor.pkl')\n",
    "# joblib.dump(best_model, 'replicas_predictor.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
