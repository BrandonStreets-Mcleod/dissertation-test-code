{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                timestamp         value\n",
      "0    03/09/2024 11:00:00  1.165827e-06\n",
      "1    03/09/2024 11:00:30  1.189352e-06\n",
      "2    03/09/2024 11:01:00  1.143872e-06\n",
      "3    03/09/2024 11:01:30  1.141328e-06\n",
      "4    03/09/2024 11:02:00  0.000000e+00\n",
      "..                   ...           ...\n",
      "353  03/09/2024 14:32:30  2.971772e-07\n",
      "354  03/09/2024 14:33:00  2.837418e-07\n",
      "355  03/09/2024 14:33:30  1.520919e-07\n",
      "356  03/09/2024 14:34:00  1.643452e-07\n",
      "357  03/09/2024 14:34:30  0.000000e+00\n",
      "\n",
      "[358 rows x 2 columns]>\n",
      "338\n",
      "(304, 20, 1)\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_10 (Conv1D)          (None, 20, 64)            256       \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 20, 32)            10272     \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 640)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 64)                41024     \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51617 (201.63 KB)\n",
      "Trainable params: 51617 (201.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8844 \n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to best_model.hdf5\n",
      "10/10 [==============================] - 3s 65ms/step - loss: 0.8844 - val_loss: 1.5125e-06\n",
      "Epoch 2/30\n",
      " 5/10 [==============>...............] - ETA: 0s - loss: 0.5890"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.4221 - val_loss: 0.0103\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1628\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1628 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0866\n",
      "Epoch 4: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.0801 - val_loss: 0.0026\n",
      "Epoch 5/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0654\n",
      "Epoch 5: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0578 - val_loss: 0.0021\n",
      "Epoch 6/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0427\n",
      "Epoch 6: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0404 - val_loss: 2.7746e-04\n",
      "Epoch 7/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0440\n",
      "Epoch 7: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0378 - val_loss: 0.0013\n",
      "Epoch 8/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0166\n",
      "Epoch 8: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0170 - val_loss: 4.3975e-04\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0186\n",
      "Epoch 9: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.0186 - val_loss: 0.0012\n",
      "Epoch 10/30\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0179\n",
      "Epoch 10: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0160 - val_loss: 1.0318e-05\n",
      "Epoch 11/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0200\n",
      "Epoch 11: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0159 - val_loss: 0.0011\n",
      "Epoch 12/30\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0106\n",
      "Epoch 12: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0107 - val_loss: 3.0430e-05\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0133\n",
      "Epoch 13: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.0133 - val_loss: 3.4004e-05\n",
      "Epoch 14/30\n",
      " 6/10 [=================>............] - ETA: 0s - loss: 0.0121\n",
      "Epoch 14: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.0097 - val_loss: 2.3872e-04\n",
      "Epoch 15/30\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0052\n",
      "Epoch 15: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.0051 - val_loss: 3.0127e-04\n",
      "Epoch 16/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0047\n",
      "Epoch 16: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0051 - val_loss: 7.8068e-05\n",
      "Epoch 17/30\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0040\n",
      "Epoch 17: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0048 - val_loss: 3.9478e-04\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0056\n",
      "Epoch 18: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0056 - val_loss: 1.9805e-06\n",
      "Epoch 19/30\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0051\n",
      "Epoch 19: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0064 - val_loss: 2.9832e-04\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 20: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0072 - val_loss: 1.0287e-04\n",
      "Epoch 21/30\n",
      " 6/10 [=================>............] - ETA: 0s - loss: 0.0071    \n",
      "Epoch 21: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.0067 - val_loss: 4.3976e-04\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0083\n",
      "Epoch 22: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.0083 - val_loss: 6.0661e-04\n",
      "Epoch 23/30\n",
      " 5/10 [==============>...............] - ETA: 0s - loss: 0.0119\n",
      "Epoch 23: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0094 - val_loss: 0.0029\n",
      "Epoch 24/30\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0048\n",
      "Epoch 24: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0060 - val_loss: 0.0023\n",
      "Epoch 25/30\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0069\n",
      "Epoch 25: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0072 - val_loss: 3.4103e-04\n",
      "Epoch 26/30\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 0.0072\n",
      "Epoch 26: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0056 - val_loss: 1.0693e-05\n",
      "Epoch 27/30\n",
      " 6/10 [=================>............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 27: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0045 - val_loss: 3.7573e-04\n",
      "Epoch 28/30\n",
      " 4/10 [===========>..................] - ETA: 0s - loss: 0.0020\n",
      "Epoch 28: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0031 - val_loss: 1.7751e-04\n",
      "Epoch 29/30\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0041\n",
      "Epoch 29: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 9.8622e-05\n",
      "Epoch 30/30\n",
      " 5/10 [==============>...............] - ETA: 0s - loss: 0.0014\n",
      "Epoch 30: val_loss did not improve from 0.00000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0031 - val_loss: 1.4294e-05\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5125e-06\n",
      "Mean Square Error: 1.512492417532485e-06\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-0.24212542 -0.24228467].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_val[ind:ind\u001b[38;5;241m+\u001b[39m(no_of_pred)]\n\u001b[0;32m     99\u001b[0m y_true\u001b[38;5;241m=\u001b[39m y_scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_true\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 100\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m y_scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(y_true,y_pred):\n\u001b[0;32m    102\u001b[0m   ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(y_true))\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1034\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1031\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1033\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1034\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1035\u001b[0m     X,\n\u001b[0;32m   1036\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1037\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1038\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1039\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\brand\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-0.24212542 -0.24228467].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from tensorflow import keras\n",
    "\n",
    "def prepare_data(seq,num):\n",
    "  x=[]\n",
    "  y=[]\n",
    "  for i in range(0,(len(seq)-num),1):\n",
    "    \n",
    "    input_ = seq[i:i+num]\n",
    "    output  = seq[i+num]\n",
    "    \n",
    "    x.append(input_)\n",
    "    y.append(output)\n",
    "    \n",
    "  return np.array(x), np.array(y)\n",
    "\n",
    "data=pd.read_csv('../Utilities\\csv\\metrics_2024-09-03\\metrics.csv')\n",
    "print(data.head)\n",
    "cpu_usage = data['value'].values\n",
    "\n",
    "num=20\n",
    "#first 10 mins of traffic\n",
    "sample = cpu_usage[:num]\n",
    "\n",
    "x,y= prepare_data(cpu_usage,num)\n",
    "print(len(x))\n",
    "\n",
    "ind = int(0.9 * len(x))\n",
    "x_tr = x[:ind]\n",
    "y_tr = y[:ind]\n",
    "x_val=x[ind:]\n",
    "y_val=y[ind:]\n",
    "\n",
    "#normalize the inputs\n",
    "x_scaler= StandardScaler()\n",
    "x_tr = x_scaler.fit_transform(x_tr)\n",
    "x_val= x_scaler.transform(x_val)\n",
    "\n",
    "#reshaping the output for normalization\n",
    "y_tr=y_tr.reshape(len(y_tr),1)\n",
    "y_val=y_val.reshape(len(y_val),1)\n",
    "\n",
    "#normalize the output\n",
    "y_scaler=StandardScaler()\n",
    "y_tr = y_scaler.fit_transform(y_tr)[:,0]\n",
    "y_val = y_scaler.transform(y_val)[:,0]\n",
    "\n",
    "#reshaping input data\n",
    "x_tr= x_tr.reshape(x_tr.shape[0],x_tr.shape[1],1)\n",
    "x_val= x_val.reshape(x_val.shape[0],x_val.shape[1],1)\n",
    "print(x_tr.shape)\n",
    "\n",
    "# define model\n",
    "model =  Sequential()\n",
    "model.add(Conv1D(64, 3, padding='same', activation='relu',input_shape=(num,1)))\n",
    "model.add(Conv1D(32, 5, padding='same', activation='relu',input_shape=(num,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(1,activation='linear'))\n",
    "model.summary()\n",
    "# Define the optimizer and loss:\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "# Define the callback to save the best model during the training\n",
    "mc = ModelCheckpoint('best_model.hdf5', monitor='val_loss', verbose=1,\n",
    "         save_best_only=True, mode='min')\n",
    "# Train the model for 30 epochs with batch size of 32:\n",
    "history=model.fit(x_tr, y_tr ,epochs=30, batch_size=32, validation_data=(x_val,y_val),\n",
    "            callbacks=[mc])\n",
    "model.load_weights('best_model.hdf5')\n",
    "mse = model.evaluate(x_val,y_val)\n",
    "print(\"Mean Square Error:\",mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
