{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                timestamp      value\n",
      "0    07/09/2024 17:00:00  14.312299\n",
      "1    07/09/2024 17:00:15  10.533495\n",
      "2    07/09/2024 17:00:30  11.239361\n",
      "3    07/09/2024 17:00:45  11.239361\n",
      "4    07/09/2024 17:01:00   9.610936\n",
      "..                   ...        ...\n",
      "276  07/09/2024 18:09:00   0.012682\n",
      "277  07/09/2024 18:09:15   0.011093\n",
      "278  07/09/2024 18:09:30   0.015599\n",
      "279  07/09/2024 18:09:45   0.012942\n",
      "280  07/09/2024 18:10:00   0.015930\n",
      "\n",
      "[281 rows x 2 columns]>\n",
      "241\n",
      "(168, 40, 1)\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_3 (Bidirecti  (None, 40, 100)           20800     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " lstm_41 (LSTM)              (None, 40, 40)            22560     \n",
      "                                                                 \n",
      " lstm_42 (LSTM)              (None, 30)                8520      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 30)                120       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52961 (206.88 KB)\n",
      "Trainable params: 52901 (206.64 KB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 30s 773ms/step - loss: 0.7890 - val_loss: 1.0835 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.3938 - val_loss: 1.0911 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.1681 - val_loss: 1.0072 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 0.1574 - val_loss: 1.0527 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 208ms/step - loss: 0.1556 - val_loss: 1.0808 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 0.1348 - val_loss: 1.0552 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.1171 - val_loss: 1.0313 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.1121 - val_loss: 1.0203 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.1344 - val_loss: 1.0157 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 247ms/step - loss: 0.1097 - val_loss: 1.0062 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 270ms/step - loss: 0.1233 - val_loss: 0.9899 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 275ms/step - loss: 0.1277 - val_loss: 0.9692 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.1569 - val_loss: 0.9581 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.1183 - val_loss: 0.9669 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 0.1295 - val_loss: 0.9848 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 214ms/step - loss: 0.1060 - val_loss: 0.9863 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.1393 - val_loss: 0.9505 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 191ms/step - loss: 0.1052 - val_loss: 0.9300 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 190ms/step - loss: 0.1192 - val_loss: 0.9304 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.1143 - val_loss: 0.9186 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 206ms/step - loss: 0.0968 - val_loss: 0.8781 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 246ms/step - loss: 0.1105 - val_loss: 0.8791 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.1037 - val_loss: 0.9105 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 0.1073 - val_loss: 0.9311 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 191ms/step - loss: 0.1049 - val_loss: 0.9075 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 0.1332 - val_loss: 0.8852 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.1270 - val_loss: 0.8667 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 323ms/step - loss: 0.1369 - val_loss: 0.8519 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 274ms/step - loss: 0.1255 - val_loss: 0.8292 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.0882 - val_loss: 0.8063 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 270ms/step - loss: 0.0925 - val_loss: 0.7990 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 0.1045 - val_loss: 0.8040 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 252ms/step - loss: 0.0927 - val_loss: 0.8165 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 276ms/step - loss: 0.0928 - val_loss: 0.8324 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 0.1748 - val_loss: 0.8276 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.1291 - val_loss: 0.8035 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.1147 - val_loss: 0.7820 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.1305 - val_loss: 0.7603 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 0.0974 - val_loss: 0.7377 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.1188 - val_loss: 0.7124 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 273ms/step - loss: 0.0891 - val_loss: 0.6956 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.1359 - val_loss: 0.6860 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.1262 - val_loss: 0.6731 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.1053 - val_loss: 0.6701 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.1149 - val_loss: 0.6738 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 204ms/step - loss: 0.0849 - val_loss: 0.6630 - lr: 1.2500e-04\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.1237 - val_loss: 0.6568 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 209ms/step - loss: 0.0762 - val_loss: 0.6508 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 0.1682 - val_loss: 0.6380 - lr: 1.2500e-04\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 0.1037 - val_loss: 0.6431 - lr: 1.2500e-04\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.1464 - val_loss: 0.6497 - lr: 1.2500e-04\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 230ms/step - loss: 0.1299 - val_loss: 0.6483 - lr: 1.2500e-04\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.1397 - val_loss: 0.6388 - lr: 1.2500e-04\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.1398 - val_loss: 0.6178 - lr: 1.2500e-04\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.1127 - val_loss: 0.5985 - lr: 1.2500e-04\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.0959 - val_loss: 0.5791 - lr: 1.2500e-04\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.1008 - val_loss: 0.5636 - lr: 1.2500e-04\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.0873 - val_loss: 0.5482 - lr: 1.2500e-04\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.1194 - val_loss: 0.5338 - lr: 1.2500e-04\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 252ms/step - loss: 0.0840 - val_loss: 0.5300 - lr: 1.2500e-04\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.1044 - val_loss: 0.5222 - lr: 1.2500e-04\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.0965 - val_loss: 0.5092 - lr: 1.2500e-04\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.0730 - val_loss: 0.4881 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.0896 - val_loss: 0.4644 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.1218 - val_loss: 0.4460 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.0806 - val_loss: 0.4210 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.1217 - val_loss: 0.4148 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.1461 - val_loss: 0.4237 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 205ms/step - loss: 0.1002 - val_loss: 0.4244 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 2s 251ms/step - loss: 0.1113 - val_loss: 0.4084 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 278ms/step - loss: 0.0940 - val_loss: 0.4026 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 250ms/step - loss: 0.0887 - val_loss: 0.4114 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 0.0875 - val_loss: 0.4125 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.0965 - val_loss: 0.4057 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 255ms/step - loss: 0.1727 - val_loss: 0.3931 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 269ms/step - loss: 0.1036 - val_loss: 0.3866 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.1054 - val_loss: 0.3730 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 2s 334ms/step - loss: 0.0792 - val_loss: 0.3570 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 285ms/step - loss: 0.0852 - val_loss: 0.3395 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 2s 323ms/step - loss: 0.1397 - val_loss: 0.3282 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 2s 299ms/step - loss: 0.0963 - val_loss: 0.3209 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 0.0954 - val_loss: 0.3171 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.1360 - val_loss: 0.3252 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.1095 - val_loss: 0.3494 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.1159 - val_loss: 0.3467 - lr: 1.2500e-04\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 267ms/step - loss: 0.0845 - val_loss: 0.3295 - lr: 1.2500e-04\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.0980 - val_loss: 0.3048 - lr: 1.2500e-04\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.0695 - val_loss: 0.2985 - lr: 1.2500e-04\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.1168 - val_loss: 0.2936 - lr: 1.2500e-04\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 204ms/step - loss: 0.0947 - val_loss: 0.2715 - lr: 1.2500e-04\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.0805 - val_loss: 0.2509 - lr: 1.2500e-04\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.1117 - val_loss: 0.2476 - lr: 1.2500e-04\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 278ms/step - loss: 0.0884 - val_loss: 0.2529 - lr: 1.2500e-04\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 0.1030 - val_loss: 0.2604 - lr: 1.2500e-04\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 209ms/step - loss: 0.1022 - val_loss: 0.2696 - lr: 1.2500e-04\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 206ms/step - loss: 0.0832 - val_loss: 0.2658 - lr: 1.2500e-04\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.1118 - val_loss: 0.2678 - lr: 1.2500e-04\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 197ms/step - loss: 0.0886 - val_loss: 0.2612 - lr: 6.2500e-05\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 246ms/step - loss: 0.0878 - val_loss: 0.2574 - lr: 6.2500e-05\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 191ms/step - loss: 0.0977 - val_loss: 0.2515 - lr: 6.2500e-05\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 1.7976\n",
      "Mean Square Error: 1.797590732574463\n",
      "Mean Square Error: 3.1757106773007537\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from tensorflow import keras\n",
    "\n",
    "def prepare_data(seq,num):\n",
    "  x=[]\n",
    "  y=[]\n",
    "  for i in range(0,(len(seq)-num),1):\n",
    "    \n",
    "    input_ = seq[i:i+num]\n",
    "    output  = seq[i+num]\n",
    "    \n",
    "    x.append(input_)\n",
    "    y.append(output)\n",
    "    \n",
    "  return np.array(x), np.array(y)\n",
    "\n",
    "data=pd.read_csv('../Utilities\\csv\\metrics_2024-09-07\\metrics.csv')\n",
    "print(data.head)\n",
    "cpu_usage = data['value'].values\n",
    "\n",
    "num=40 # The first 10 mins of traffic\n",
    "sample = cpu_usage[:num]\n",
    "\n",
    "x,y= prepare_data(cpu_usage,num)\n",
    "print(len(x))\n",
    "\n",
    "ind = int(0.7 * len(x))\n",
    "x_tr = x[:ind]\n",
    "y_tr = y[:ind]\n",
    "x_val=x[ind:]\n",
    "y_val=y[ind:]\n",
    "\n",
    "#normalize the inputs\n",
    "x_scaler= StandardScaler()\n",
    "x_tr = x_scaler.fit_transform(x_tr)\n",
    "x_val= x_scaler.transform(x_val)\n",
    "\n",
    "#reshaping the output for normalization\n",
    "y_tr=y_tr.reshape(len(y_tr),1)\n",
    "y_val=y_val.reshape(len(y_val),1)\n",
    "\n",
    "#normalize the output\n",
    "y_scaler=StandardScaler()\n",
    "y_tr = y_scaler.fit_transform(y_tr)[:,0]\n",
    "y_val = y_scaler.transform(y_val)[:,0]\n",
    "\n",
    "#reshaping input data\n",
    "x_tr= x_tr.reshape(x_tr.shape[0],x_tr.shape[1],1)\n",
    "x_val= x_val.reshape(x_val.shape[0],x_val.shape[1],1)\n",
    "print(x_tr.shape)\n",
    "\n",
    "def traffic_prediction_lstm():\n",
    "    model = Sequential()\n",
    "    # Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), input_shape=(num, 1)))\n",
    "    # Additional LSTM layers\n",
    "    model.add(LSTM(40, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(30, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # Batch Normalization\n",
    "    model.add(BatchNormalization())\n",
    "    # Dense layer with ReLU activation\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    # Output layer for regression\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # Learning Rate Scheduler\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    return model, lr_scheduler\n",
    "\n",
    "\n",
    "lstm_model, lr_scheduler = traffic_prediction_lstm()\n",
    "# Early Stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = lstm_model.fit(x_tr, y_tr,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "mse = lstm_model.evaluate(x_val,y_val)\n",
    "print(\"Mean Square Error:\",mse)\n",
    "\n",
    "#build a simple model\n",
    "def compute_moving_average(data):\n",
    "  pred=[]\n",
    "  for i in data:\n",
    "    avg=np.sum(i)/len(i)\n",
    "    pred.append(avg)\n",
    "  return np.array(pred)\n",
    "x_reshaped = x_val.reshape(-1,40)\n",
    "y_pred = compute_moving_average(x_reshaped)\n",
    "mse = np.sum ( (y_val - y_pred) **2 ) / (len(y_val))\n",
    "print(\"Mean Square Error:\",mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
