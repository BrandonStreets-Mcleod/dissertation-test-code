{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: -10939.112756978408 is not greater than 65343.18060707973. Delta is -76282.29336405813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (22042, 60, 11)\n",
      "Test shapes: (22042, 10)\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 50ms/step\n",
      "Model 1 - RMSE: 16.74983787536621, MSE: 280.5570373535156\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step\n",
      "Model 2 - RMSE: 36.06755065917969, MSE: 1300.8682861328125\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step\n",
      "Model 3 - RMSE: 33.975868225097656, MSE: 1154.3597412109375\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step\n",
      "Model 4 - RMSE: 81.28636169433594, MSE: 6607.47314453125\n",
      "Best performing model: best_model.keras with RMSE: 16.74983787536621\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import *\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import *\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import *\n",
    "from scipy import interpolate\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pandas import to_numeric\n",
    "from hmmlearn import hmm\n",
    "from keras.losses import *\n",
    "from dateutil import parser\n",
    "\n",
    "def create_time_features(data):\n",
    "    # Assume 'cpu_data' is your dataframe and 'datetime' is the column with datetime values\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])  # Ensure it's a datetime type\n",
    "    data['hour'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    data['day_of_month'] = data['datetime'].dt.day\n",
    "    data['is_weekend'] = data['datetime'].dt.weekday >= 5  # True for Saturday and Sunday\n",
    "    return data\n",
    "\n",
    "# Define a function to create lagged features\n",
    "def create_additional_features(df, target_col, lags):\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    df['cpu_change'] = df['cpu_usage'].diff() # Change in CPU usage\n",
    "    df['moving_avg'] = df['cpu_usage'].rolling(window=10).mean() # 10-minute moving average\n",
    "    return df\n",
    "\n",
    "cpu_data = pd.read_csv('../Utilities\\csv/cpu_usage_data_test.csv')\n",
    "# Convert 'datetime' column to datetime objects\n",
    "cpu_data['datetime'] = pd.to_datetime(cpu_data['datetime'])\n",
    "\n",
    "# Set 'datetime' as the index\n",
    "cpu_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Resample to 1-minute intervals and interpolate\n",
    "cpu_data = cpu_data.resample('1T').interpolate(method='linear')\n",
    "cpu_data.reset_index(inplace=True)\n",
    "if 'machine_id' in cpu_data.columns:\n",
    "    cpu_data['machine_id'] = pd.Categorical(cpu_data['machine_id']).codes + 1\n",
    "else:\n",
    "    cpu_data['machine_id'] = 1\n",
    "cpu_data = create_time_features(cpu_data)\n",
    "lags = [5, 10, 60]\n",
    "cpu_data = create_additional_features(cpu_data, 'cpu_usage', lags)\n",
    "\n",
    "# Drop any rows with NaN values that were created due to shifting\n",
    "cpu_data.dropna(inplace=True)\n",
    "\n",
    "column_to_move = cpu_data.pop('cpu_usage')\n",
    "cpu_data.insert(1, 'cpu_usage', column_to_move)\n",
    "all_features_no_timestamp = cpu_data.columns[1:len(cpu_data.columns)]\n",
    "# Extract the relevant features from your dataset for the HMM\n",
    "features_for_hmm = cpu_data[all_features_no_timestamp].values\n",
    "\n",
    "# Define and fit the HMM model\n",
    "num_states = 10  # You can adjust this based on the nature of the data\n",
    "model = hmm.GaussianHMM(n_components=num_states, covariance_type=\"full\", n_iter=1000)\n",
    "\n",
    "# Fit the HMM model to the features\n",
    "model.fit(features_for_hmm)\n",
    "\n",
    "# Predict hidden states for each time step\n",
    "hidden_states = model.predict(features_for_hmm)\n",
    "\n",
    "cpu_data['hidden_state'] = hidden_states\n",
    "\n",
    "cpu_data.head\n",
    "\n",
    "# Initialize scalers\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Window size (e.g., past 60 time steps) and prediction horizon (next 10 timesteps)\n",
    "window_size = 60\n",
    "prediction_horizon = 10\n",
    "\n",
    "# Function to generate LSTM sequences\n",
    "def create_lstm_sequences(group_data, window_size, prediction_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(group_data) - window_size - prediction_horizon + 1):\n",
    "        # Features are all columns except 'datetime', 'cpu_usage', and 'machine_id'\n",
    "        X.append(group_data.iloc[i:i + window_size].drop(columns=['datetime', 'cpu_usage', 'machine_id']).values)\n",
    "        # Target is the next 10 CPU usage values\n",
    "        y.append(group_data.iloc[i + window_size:i + window_size + prediction_horizon]['cpu_usage'].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "all_features_no_timestamp_cpu_usage = cpu_data.columns[2:len(cpu_data.columns)]\n",
    "# Scale features and target (fit on the entire dataset)\n",
    "cpu_data[all_features_no_timestamp_cpu_usage] = feature_scaler.fit_transform(cpu_data[all_features_no_timestamp_cpu_usage])\n",
    "cpu_data[[\"cpu_usage\"]] = target_scaler.fit_transform(cpu_data[[\"cpu_usage\"]])\n",
    "\n",
    "X_test, y_test = create_lstm_sequences(cpu_data, window_size, prediction_horizon)\n",
    "\n",
    "# Print the shapes of the final train/test data\n",
    "print(\"Train shapes:\", X_test.shape)\n",
    "print(\"Test shapes:\", y_test.shape)\n",
    "\n",
    "# Define paths to the models\n",
    "model_paths = [\n",
    "    'best_model.keras',\n",
    "    'best_model_idea.keras',\n",
    "    'best_model_2.keras',\n",
    "    'best_model_colab.keras'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store model performances\n",
    "model_performance = {}\n",
    "\n",
    "# Loop over each model\n",
    "for i, model_path in enumerate(model_paths):\n",
    "    # Load the model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions and actual values if necessary\n",
    "    y_pred_unscaled = target_scaler.inverse_transform(y_pred)\n",
    "    y_test_unscaled = target_scaler.inverse_transform(y_test)\n",
    "    \n",
    "    # Calculate MSE and RMSE\n",
    "    mse = mean_squared_error(y_test_unscaled.flatten(), y_pred_unscaled.flatten())\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Store the results\n",
    "    model_performance[f'model_{i+1}'] = {'MSE': mse, 'RMSE': rmse, 'Name': model_path}\n",
    "\n",
    "    # Print or log performance for this model\n",
    "    print(f\"Model {i+1} - RMSE: {rmse}, MSE: {mse}\")\n",
    "\n",
    "# Optionally, you can find the best performing model\n",
    "best_model = min(model_performance, key=lambda x: model_performance[x]['RMSE'])\n",
    "print(f\"Best performing model: {model_performance[best_model]['Name']} with RMSE: {model_performance[best_model]['RMSE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST_MODEL_IDEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (160742, 60, 11) (160742, 10)\n",
      "Test shapes: (39825, 60, 11) (39825, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">175,700</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m125\u001b[0m)        │         \u001b[38;5;34m4,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m125\u001b[0m)        │           \u001b[38;5;34m500\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m75\u001b[0m)         │        \u001b[38;5;34m28,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m75\u001b[0m)         │           \u001b[38;5;34m300\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m75\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m175\u001b[0m)        │       \u001b[38;5;34m175,700\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m175\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m110,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m510\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">350,060</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m350,060\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,660</span> (1.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m349,660\u001b[0m (1.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> (1.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m400\u001b[0m (1.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 127ms/step - loss: 0.0041 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 138ms/step - loss: 0.0021 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 140ms/step - loss: 0.0018 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 163ms/step - loss: 0.0017 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 153ms/step - loss: 0.0016 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 149ms/step - loss: 0.0015 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 137ms/step - loss: 0.0014 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 135ms/step - loss: 0.0013 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m2512/2512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 135ms/step - loss: 0.0012 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
      "\u001b[1m1245/1245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 43ms/step - loss: 0.0018\n",
      "RMSE: 0.03777431963104573, MSE: 0.001426899223588407\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "cpu_data = pd.read_csv('../Utilities/csv/cpu_usage_train.csv')\n",
    "\n",
    "# Initialize scalers\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Window size (e.g., past 60 time steps) and prediction horizon (next 10 timesteps)\n",
    "window_size = 60\n",
    "prediction_horizon = 10\n",
    "\n",
    "# Function to generate LSTM sequences\n",
    "def create_lstm_sequences(group_data, window_size, prediction_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(group_data) - window_size - prediction_horizon + 1):\n",
    "        # Features are all columns except 'datetime', 'cpu_usage', and 'machine_id'\n",
    "        X.append(group_data.iloc[i:i + window_size].drop(columns=['datetime', 'cpu_usage', 'machine_id']).values)\n",
    "        # Target is the next 10 CPU usage values\n",
    "        y.append(group_data.iloc[i + window_size:i + window_size + prediction_horizon]['cpu_usage'].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "all_features_no_timestamp_cpu_usage = cpu_data.columns[2:len(cpu_data.columns)]\n",
    "# Scale features and target (fit on the entire dataset)\n",
    "cpu_data[all_features_no_timestamp_cpu_usage] = feature_scaler.fit_transform(cpu_data[all_features_no_timestamp_cpu_usage])\n",
    "cpu_data[[\"cpu_usage\"]] = target_scaler.fit_transform(cpu_data[[\"cpu_usage\"]])\n",
    "\n",
    "# Train/test split ratios\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Lists to store train/test data\n",
    "X_train_all, y_train_all = [], []\n",
    "X_test_all, y_test_all = [], []\n",
    "\n",
    "# Group by machine_id\n",
    "for machine_id, group_data in cpu_data.groupby('machine_id'):\n",
    "    # Split the data into train and test sets for this machine\n",
    "    train_data, test_data = train_test_split(group_data, test_size=1 - train_ratio, shuffle=False)\n",
    "    \n",
    "    # Generate moving windows for the training set\n",
    "    X_train, y_train = create_lstm_sequences(train_data, window_size, prediction_horizon)\n",
    "    X_train_all.append(X_train)\n",
    "    y_train_all.append(y_train)\n",
    "    \n",
    "    # Generate moving windows for the test set\n",
    "    X_test, y_test = create_lstm_sequences(test_data, window_size, prediction_horizon)\n",
    "    X_test_all.append(X_test)\n",
    "    y_test_all.append(y_test)\n",
    "\n",
    "# Combine all the machine-specific windows together for training and testing\n",
    "X_train_all = np.concatenate(X_train_all, axis=0)\n",
    "y_train_all = np.concatenate(y_train_all, axis=0)\n",
    "X_test_all = np.concatenate(X_test_all, axis=0)\n",
    "y_test_all = np.concatenate(y_test_all, axis=0)\n",
    "\n",
    "# Print the shapes of the final train/test data\n",
    "print(\"Train shapes:\", X_train_all.shape, y_train_all.shape)\n",
    "print(\"Test shapes:\", X_test_all.shape, y_test_all.shape)\n",
    "\n",
    "# Build the enhanced CNN-LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Input(shape=(X_train_all.shape[1], X_train_all.shape[2])))\n",
    "# CNN layers\n",
    "lstm_model.add(Conv1D(filters=125, kernel_size=3, activation='relu'))\n",
    "lstm_model.add(BatchNormalization())  # Normalize activations\n",
    "lstm_model.add(Conv1D(filters=75, kernel_size=3, activation='relu'))\n",
    "lstm_model.add(BatchNormalization())  # Normalize activations\n",
    "lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "# LSTM layers\n",
    "lstm_model.add(LSTM(units=175, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.4))\n",
    "lstm_model.add(LSTM(units=100, return_sequences=True))\n",
    "lstm_model.add(Dropout(0.4))\n",
    "lstm_model.add(LSTM(units=50, return_sequences=False))\n",
    "# Output layer (multi-step prediction)\n",
    "lstm_model.add(Dense(prediction_horizon))\n",
    "\n",
    "# Early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model_idea.keras', monitor='val_loss', save_best_only=True, mode='min')\n",
    "# Learning rate reduction callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "lstm_model.compile(optimizer=\"adam\", loss=Huber())  # Use Huber loss to reduce outliers' impact\n",
    "lstm_model.summary()\n",
    "# Train the model\n",
    "history = lstm_model.fit(X_train_all, y_train_all, epochs=50, batch_size=64, validation_data=(X_test_all, y_test_all),\n",
    "                         verbose=1, callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "mse_lstm = lstm_model.evaluate(X_test_all, y_test_all)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "\n",
    "print(f\"RMSE: {rmse_lstm}, MSE: {mse_lstm}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
